version: "3.8"

# =============================================================================
# Vending Machine Development Environment
# =============================================================================
#
# Profiles:
#   (default)     - postgres + server + ml-server (CPU)
#   --profile gpu - Use GPU-enabled ML server instead
#   --profile dev - Add hot reload, debug ports
#   --profile tools - Add pgAdmin, Redis Commander
#   --profile full - All services including tools
#
# Usage:
#   docker compose up -d                    # Default stack
#   docker compose --profile gpu up -d      # With GPU ML server
#   docker compose --profile dev up -d      # Development mode
#   docker compose --profile tools up -d    # With admin tools
#   docker compose --profile full up -d     # Everything
#
# =============================================================================

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

x-healthcheck-defaults: &healthcheck-defaults
  interval: 10s
  timeout: 5s
  retries: 5
  start_period: 10s

services:
  # ===========================================================================
  # Core Services
  # ===========================================================================

  postgres:
    image: postgres:16-alpine
    container_name: vending-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-vending}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-vending}
      POSTGRES_DB: ${POSTGRES_DB:-vending}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/db/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-vending} -d ${POSTGRES_DB:-vending}"]
    restart: unless-stopped
    logging: *default-logging

  server:
    build:
      context: ./server
      dockerfile: Dockerfile
      target: ${SERVER_BUILD_TARGET:-production}
    container_name: vending-server
    ports:
      - "${SERVER_PORT:-8080}:8080"
    environment:
      - PORT=8080
      - DATABASE_URL=postgres://${POSTGRES_USER:-vending}:${POSTGRES_PASSWORD:-vending}@postgres:5432/${POSTGRES_DB:-vending}?sslmode=disable
      - ML_SERVER_ADDRESS=${ML_SERVER_ADDRESS:-ml-server:50051}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - GIN_MODE=${GIN_MODE:-release}
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
    restart: unless-stopped
    logging: *default-logging

  ml-server:
    build:
      context: ./ml
      dockerfile: Dockerfile
    container_name: vending-ml-server
    profiles:
      - ""  # Include in default profile
    ports:
      - "${ML_SERVER_PORT:-50051}:50051"
    volumes:
      - ./ml/models:/app/models:ro
    environment:
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50051
      - LOG_LEVEL=${ML_LOG_LEVEL:-INFO}
      - MODEL_DIR=/app/models
      - MODEL_NAME=${ML_MODEL_NAME:-best.pt}
      - MODEL_WATCH_INTERVAL=${ML_WATCH_INTERVAL:-5.0}
      - DEFAULT_CONFIDENCE=${ML_CONFIDENCE:-0.5}
    healthcheck:
      <<: *healthcheck-defaults
      interval: 30s
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50051'); grpc.channel_ready_future(channel).result(timeout=5)"]
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  # ===========================================================================
  # GPU-enabled ML Server (use --profile gpu)
  # ===========================================================================

  ml-server-gpu:
    build:
      context: ./ml
      dockerfile: Dockerfile
    container_name: vending-ml-server-gpu
    profiles:
      - gpu
      - full
    ports:
      - "${ML_SERVER_PORT:-50051}:50051"
    volumes:
      - ./ml/models:/app/models:ro
    environment:
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50051
      - LOG_LEVEL=${ML_LOG_LEVEL:-INFO}
      - MODEL_DIR=/app/models
      - MODEL_NAME=${ML_MODEL_NAME:-best.pt}
      - MODEL_WATCH_INTERVAL=${ML_WATCH_INTERVAL:-5.0}
      - DEFAULT_CONFIDENCE=${ML_CONFIDENCE:-0.5}
    healthcheck:
      <<: *healthcheck-defaults
      interval: 30s
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50051'); grpc.channel_ready_future(channel).result(timeout=5)"]
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ===========================================================================
  # Development Services (use --profile dev)
  # ===========================================================================

  server-dev:
    build:
      context: ./server
      dockerfile: Dockerfile.dev
    container_name: vending-server-dev
    profiles:
      - dev
    ports:
      - "${SERVER_PORT:-8080}:8080"
      - "2345:2345"  # Delve debugger
    environment:
      - PORT=8080
      - DATABASE_URL=postgres://${POSTGRES_USER:-vending}:${POSTGRES_PASSWORD:-vending}@postgres:5432/${POSTGRES_DB:-vending}?sslmode=disable
      - ML_SERVER_ADDRESS=${ML_SERVER_ADDRESS:-ml-server:50051}
      - LOG_LEVEL=debug
      - GIN_MODE=debug
    volumes:
      - ./server:/app:cached
      - go_mod_cache:/go/pkg/mod
      - go_build_cache:/root/.cache/go-build
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  ml-server-dev:
    build:
      context: ./ml
      dockerfile: Dockerfile.dev
    container_name: vending-ml-server-dev
    profiles:
      - dev
    ports:
      - "${ML_SERVER_PORT:-50051}:50051"
      - "5678:5678"  # debugpy
    volumes:
      - ./ml/server:/app/server:cached
      - ./ml/models:/app/models
      - ./ml/proto:/app/proto:ro
    environment:
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50051
      - LOG_LEVEL=DEBUG
      - MODEL_DIR=/app/models
      - MODEL_NAME=${ML_MODEL_NAME:-best.pt}
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
    restart: unless-stopped

  # ===========================================================================
  # Admin Tools (use --profile tools)
  # ===========================================================================

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: vending-pgadmin
    profiles:
      - tools
      - full
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@vending.local}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
    ports:
      - "${PGADMIN_PORT:-5050}:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./scripts/db/pgadmin-servers.json:/pgadmin4/servers.json:ro
    depends_on:
      - postgres
    restart: unless-stopped
    logging: *default-logging

  redis:
    image: redis:7-alpine
    container_name: vending-redis
    profiles:
      - tools
      - full
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "redis-cli", "ping"]
    restart: unless-stopped
    logging: *default-logging

  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: vending-redis-commander
    profiles:
      - tools
      - full
    environment:
      - REDIS_HOSTS=local:redis:6379
    ports:
      - "${REDIS_COMMANDER_PORT:-8081}:8081"
    depends_on:
      - redis
    restart: unless-stopped
    logging: *default-logging

  # ===========================================================================
  # ML Training (one-shot container)
  # ===========================================================================

  ml-train:
    build:
      context: ./ml
      dockerfile: Dockerfile.train
    container_name: vending-ml-train
    profiles:
      - train
    volumes:
      - ./ml/data:/workspace/data
      - ./ml/models:/workspace/models
      - ./ml/runs:/workspace/runs
      - ./ml/training/configs:/workspace/training/configs
    environment:
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: ["--data", "/workspace/training/configs/beverages.yaml", "--epochs", "100"]

# =============================================================================
# Volumes
# =============================================================================

volumes:
  postgres_data:
    name: vending_postgres_data
  pgadmin_data:
    name: vending_pgadmin_data
  redis_data:
    name: vending_redis_data
  go_mod_cache:
    name: vending_go_mod_cache
  go_build_cache:
    name: vending_go_build_cache

# =============================================================================
# Networks
# =============================================================================

networks:
  default:
    name: vending-network
